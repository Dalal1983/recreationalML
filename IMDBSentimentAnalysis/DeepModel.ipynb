{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Using Word2Vec and an RNN</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boniface/.local/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import word2vec, Word2Vec\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops.rnn_cell import GRUCell, DropoutWrapper, MultiRNNCell\n",
    "from tensorflow.python.ops.nn import relu, elu, softmax\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.python.framework.ops import reset_default_graph\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lab = pd.read_csv( \"labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlab = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the dataset and processing it to use Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(raw_review):\n",
    "    \"\"\"Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw movie review), and \n",
    "    the output is a single string (a preprocessed movie review)\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letter characters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4.Return the result.\n",
    "    return words\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences( review, tokenizer):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boniface/.local/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/boniface/anaconda3/envs/tensorflow/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/home/boniface/.local/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/home/boniface/.local/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "print (\"Parsing sentences from training set\")\n",
    "for review in lab.review:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlab.review:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# sert Ã  sortir de jolis messages de temps en temps\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window sized\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, \\\n",
    "                          window = context, sample = downsampling)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.doesnt_match('man woman elderly children spoon'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6198972463607788),\n",
       " ('lady', 0.5801385045051575),\n",
       " ('lad', 0.5663866996765137),\n",
       " ('monk', 0.5272423624992371),\n",
       " ('farmer', 0.5249172449111938),\n",
       " ('men', 0.5218124985694885),\n",
       " ('millionaire', 0.5207440853118896),\n",
       " ('soldier', 0.517442524433136),\n",
       " ('guy', 0.5158977508544922),\n",
       " ('person', 0.5004771947860718)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spain', 0.8204919695854187),\n",
       " ('italy', 0.8125512003898621),\n",
       " ('germany', 0.8049379587173462),\n",
       " ('england', 0.7958934307098389),\n",
       " ('europe', 0.790851354598999),\n",
       " ('india', 0.7580022811889648),\n",
       " ('greece', 0.7553827166557312),\n",
       " ('poland', 0.7497527599334717),\n",
       " ('austria', 0.7405729293823242),\n",
       " ('northern', 0.7390496134757996)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.5144521594047546),\n",
       " ('queen', 0.5030089616775513),\n",
       " ('prince', 0.4746342897415161),\n",
       " ('elizabeth', 0.45409542322158813),\n",
       " ('rudolf', 0.451641321182251),\n",
       " ('bride', 0.4308673143386841),\n",
       " ('juliet', 0.42891353368759155),\n",
       " ('lion', 0.42094939947128296),\n",
       " ('stella', 0.41837969422340393),\n",
       " ('victoria', 0.4146151542663574)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59717562174767336"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('running', 'run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63969969394729009"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('running', 'walking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the embeddings to get the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('300features_40minwords_10context') # load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16490, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape # numpy array containing the embeddings number of words x embedding dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\")) # searching a set is much faster than a list\n",
    "\n",
    "def review_to_words(raw_review):\n",
    "    \"\"\"Function to convert a raw review to a string of words\n",
    "    The input is a single string (a raw movie review), and \n",
    "    the output is a single string (a preprocessed movie review)\n",
    "    \"\"\"\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return the result.\n",
    "    return meaningful_words\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    \"\"\"Loops the previous function to clean an entire dataset\"\"\"\n",
    "        \n",
    "    new = dataset\n",
    "    revs = []\n",
    "    for rev in new.review:\n",
    "        revs.append(review_to_words(rev))\n",
    "        \n",
    "    new.review = revs \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boniface/.local/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /home/boniface/anaconda3/envs/tensorflow/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "data = clean_dataset(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, train_size=0.8, test_size=0.2, random_state=42)\n",
    "x_train, y_train = train.review, train.sentiment\n",
    "x_val, y_val = val.review, val.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_val, y_val = np.array(x_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embed_reviews(model, reviews):\n",
    "    \"\"\"Replaces each word by its embedding.\"\"\"\n",
    "    all_reviews = []\n",
    "    for rev in reviews:\n",
    "        embed = []\n",
    "        for word in rev:\n",
    "            if word in model.vocab.keys(): # only words that appear at least 40 times have an embedding\n",
    "                embed.append(model[word])\n",
    "            else:\n",
    "                embed.append(model['the']) # I tried to take the least significant word but not sure if it's a good idea\n",
    "        \n",
    "        embed = np.array(embed)\n",
    "        all_reviews.append(embed)\n",
    "        \n",
    "    return np.array(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = embed_reviews(model, x_train)\n",
    "x_val = embed_reviews(model, x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad(features, batch_size):\n",
    "    \"\"\"Pads the features with zeros, so that we can have a tensor.\"\"\"\n",
    "    lmax = 0\n",
    "    for line in features:\n",
    "        if line.shape[0] > lmax:\n",
    "            lmax = line.shape[0]\n",
    "\n",
    "    new = np.zeros((batch_size, lmax, 300))\n",
    "    for i,sample in enumerate(features):\n",
    "        for j,word in enumerate(sample):\n",
    "            for k,coeff in enumerate(word):\n",
    "                new[i,j,k] = coeff\n",
    "                \n",
    "    return new\n",
    "\n",
    "def onehot(t, n_classes):\n",
    "    \"\"\"Does the one-hot encoding\"\"\"\n",
    "    out = np.zeros((t.shape[0], n_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "#n_layers = 2\n",
    "n_hidden = 128\n",
    "state_size = 64\n",
    "\n",
    "reset_default_graph()\n",
    "\n",
    "x_pl = tf.placeholder(tf.float32, [None, None, 300])\n",
    "\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2)) # put a 1 where there is a non-zero value\n",
    "    length = tf.reduce_sum(used, reduction_indices=1) # sums the ones to obtain the actual sequence length\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "\n",
    "cell = GRUCell(state_size)\n",
    "#cell = DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "#cell = MultiRNNCell([cell] * n_layers) # in order to take into account different time steps\n",
    "\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, x_pl, sequence_length=length(x_pl), dtype=tf.float32, scope='unfold')\n",
    "# when the argument sequence_length is given, state is the last RELEVANT state\n",
    "\n",
    "fc = fully_connected(state, n_hidden, activation_fn=elu, scope='fc')\n",
    "y = fully_connected(fc, n_classes, activation_fn=softmax, scope='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "learning_rate = 1e-4\n",
    "\n",
    "y_pl = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_pl * tf.log(y+epsilon))\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "# maybe we should bring the gradient norm to 1\n",
    "train_op = optimizer.minimize(cross_entropy)\n",
    "\n",
    "correct_predictions = tf.equal(tf.argmax(y, 1), tf.argmax(y_pl, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "auc = tf.contrib.metrics.streaming_auc(y, y_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "\n",
    "n_batches_train = len(x_train) // batch_size\n",
    "n_batches_val = len(x_val) // batch_size\n",
    "\n",
    "train_acc, train_auc, train_loss = [], [], []\n",
    "val_acc, val_auc, val_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer()) # needed for the auc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean batch loss train: 30.3162347317 Mean batch loss valid: 29.7588188934\n",
      "Train accuracy: 0.875450002253 Valid accuracy: 0.871799999475\n",
      "Train ROC auc score: 0.936852068603 Valid ROC auc score: 0.937307502031\n",
      "Mean batch loss train: 30.1046811581 Mean batch loss valid: 29.5837371445\n",
      "Train accuracy: 0.876150001585 Valid accuracy: 0.871799998283\n",
      "Train ROC auc score: 0.937853122056 Valid ROC auc score: 0.938240491152\n",
      "Mean batch loss train: 29.9094496632 Mean batch loss valid: 29.4240163803\n",
      "Train accuracy: 0.87650000155 Valid accuracy: 0.872799999714\n",
      "Train ROC auc score: 0.938713358641 Valid ROC auc score: 0.939048223495\n",
      "Mean batch loss train: 29.7430462837 Mean batch loss valid: 29.2907396698\n",
      "Train accuracy: 0.87715000093 Valid accuracy: 0.87439999938\n",
      "Train ROC auc score: 0.939463941157 Valid ROC auc score: 0.939758183956\n",
      "Mean batch loss train: 29.5887020111 Mean batch loss valid: 29.168461113\n",
      "Train accuracy: 0.877300000787 Valid accuracy: 0.877199997902\n",
      "Train ROC auc score: 0.940127744973 Valid ROC auc score: 0.940389553308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a38199aa0f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we do the padding per batch for memory efficience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mfetches_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-cae03e46e6b5>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(features, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcoeff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    for i in range(n_batches_train):\n",
    "        idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = pad(x_train[idx], batch_size) # we do the padding per batch for memory efficience\n",
    "        y_batch = onehot(y_train[idx], n_classes)\n",
    "        fetches_train = [train_op]\n",
    "        feed_train = {x_pl: x_batch, y_pl: y_batch}\n",
    "        sess.run(fetches_train, feed_train)\n",
    "    \n",
    "    cur_loss = 0\n",
    "    cur_acc = 0\n",
    "    cur_auc = 0\n",
    "    for i in range(n_batches_train):\n",
    "        idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = pad(x_train[idx], batch_size)\n",
    "        y_batch = onehot(y_train[idx], n_classes)\n",
    "        fetches_eval_train = [cross_entropy, accuracy, auc]\n",
    "        feed_eval_train = {x_pl: x_batch, y_pl: y_batch}\n",
    "        batch_loss, batch_acc, batch_auc = sess.run(fetches_eval_train, feed_eval_train)\n",
    "        batch_auc, _ = batch_auc # it's a tupple that contains twice the same value\n",
    "        cur_loss += batch_loss\n",
    "        cur_acc += batch_acc\n",
    "        cur_auc += batch_auc\n",
    "        \n",
    "    train_loss.append(cur_loss/n_batches_train)\n",
    "    train_acc.append(cur_acc/n_batches_train)\n",
    "    train_auc.append(cur_auc/n_batches_train)\n",
    "    \n",
    "    cur_loss = 0\n",
    "    cur_acc = 0\n",
    "    cur_auc = 0\n",
    "    for i in range(n_batches_val):\n",
    "        idx = np.arange(i*batch_size, (i+1)*batch_size)\n",
    "        x_batch = pad(x_val[idx], batch_size)\n",
    "        y_batch = onehot(y_val[idx], n_classes)\n",
    "        fetches_eval_val = [cross_entropy, accuracy, auc]\n",
    "        feed_eval_val = {x_pl: x_batch, y_pl: y_batch}\n",
    "        batch_loss, batch_acc, batch_auc = sess.run(fetches_eval_val, feed_eval_val)\n",
    "        batch_auc, _ = batch_auc\n",
    "        cur_loss += batch_loss\n",
    "        cur_acc += batch_acc\n",
    "        cur_auc += batch_auc\n",
    "            \n",
    "    val_loss.append(cur_loss/n_batches_val)\n",
    "    val_acc.append(cur_acc/n_batches_val)\n",
    "    val_auc.append(cur_auc/n_batches_val)\n",
    "    \n",
    "    print('Mean batch loss train:', train_loss[-1], 'Mean batch loss valid:', val_loss[-1])\n",
    "    print('Train accuracy:', train_acc[-1], 'Valid accuracy:', val_acc[-1])\n",
    "    print('Train ROC auc score:', train_auc[-1], 'Valid ROC auc score:', val_auc[-1])\n",
    "    \n",
    "    if len(val_auc) > 1 and val_auc[-1] < val_auc[-2]:\n",
    "        print('Early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is very computationnaly intensive, so I will test it later on AWS EC2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
